# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGsoEawIHFPDmwmCDxXsy2ArDyoLfQAp
"""

# CodeGrade Tag Init1

from google.colab import drive
drive.mount('/content/drive')

"""Import Libraries"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50
from tensorflow.keras.optimizers import RMSprop, Adam, SGD, Adagrad
from numba import jit
import matplotlib.pyplot as plt
import os

""" Data Preprocess"""

# Function to normalize data
@jit(nopython=True)
def normalize_data(data):
    mean = np.mean(data, axis=(0, 1, 2))
    std = np.std(data, axis=(0, 1, 2))
    return (data - mean) / std
# Define batch size
batch_size = 32

# Define paths
dataset_dir = '/content/drive/MyDrive/Archive'
train_data_dir = '/content/drive/MyDrive/Archive/Training'
test_data_dir = '/content/drive/MyDrive/Archive/Testing'

os.makedirs(train_data_dir, exist_ok=True)
os.makedirs(test_data_dir, exist_ok=True)

for category in os.listdir(dataset_dir):
    category_path = os.path.join(dataset_dir, category)
    if os.path.isdir(category_path):
        images = [f for f in os.listdir(category_path) if os.path.isfile(os.path.join(category_path, f))]
        if len(images) > 0:
            random.shuffle(images)
            selected_images = images[:25]  # Select only 25 images
            train_images, test_images = train_test_split(selected_images, test_size=0.2, random_state=42)

            os.makedirs(os.path.join(train_data_dir, category), exist_ok=True)
            os.makedirs(os.path.join(test_data_dir, category), exist_ok=True)

            for img in train_images:
                shutil.copy(os.path.join(category_path, img), os.path.join(train_data_dir, category, img))

            for img in test_images:
                shutil.copy(os.path.join(category_path, img), os.path.join(test_data_dir, category, img))

"""Image Data Generators"""

train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_data_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

"""Create Model Function"""

# Build VGG16 model from scratch
vgg16_model_scratch = Sequential([
    Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(256, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(4, activation='softmax')  # 4 classes: glioma, meningioma, pituitary, no_tumor
])

# Load pre-trained VGG16 model without top layers
base_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add custom top layers
x = base_vgg16.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
predictions_vgg16 = Dense(4, activation='softmax')(x)

# Final VGG16 transfer model
vgg16_model_transfer = Model(inputs=base_vgg16.input, outputs=predictions_vgg16)

# Load pre-trained InceptionV3 model without top layers
base_inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add custom top layers
x = base_inception.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
predictions_inception = Dense(4, activation='softmax')(x)

# Final InceptionV3 model
inception_model = Model(inputs=base_inception.input, outputs=predictions_inception)

# Load pre-trained ResNet50 model without top layers
base_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add custom top layers
x = base_resnet.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
predictions_resnet = Dense(4, activation='softmax')(x)

# Final ResNet50 model
resnet_model = Model(inputs=base_resnet.input, outputs=predictions_resnet)

# Define optimizers
optimizer_vgg16_scratch = RMSprop(learning_rate=0.001)
optimizer_vgg16_transfer = Adam(learning_rate=0.001)
optimizer_inception = SGD(learning_rate=0.001, momentum=0.9)
optimizer_resnet = Adagrad(learning_rate=0.001)

# Compile models with their respective optimizers
vgg16_model_scratch.compile(loss='categorical_crossentropy',
                            optimizer=optimizer_vgg16_scratch,
                            metrics=['accuracy'])

vgg16_model_transfer.compile(optimizer=optimizer_vgg16_transfer,
                             loss='categorical_crossentropy',
                             metrics=['accuracy'])

inception_model.compile(optimizer=optimizer_inception,
                        loss='categorical_crossentropy',
                        metrics=['accuracy'])

resnet_model.compile(optimizer=optimizer_resnet,
                     loss='categorical_crossentropy',
                     metrics=['accuracy'])

# Summarize models
vgg16_model_scratch.summary()

vgg16_model_transfer.summary()

inception_model.summary()

resnet_model.summary()

# Train the models
epochs = 20

history_vgg16_scratch = vgg16_model_scratch.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    validation_data=test_generator,
    validation_steps=test_generator.samples // batch_size,
    epochs=epochs
)

history_vgg16_transfer = vgg16_model_transfer.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    validation_data=test_generator,
    validation_steps=test_generator.samples // batch_size,
    epochs=epochs
)

history_inception = inception_model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    validation_data=test_generator,
    validation_steps=test_generator.samples // batch_size,
    epochs=epochs
)

history_resnet = resnet_model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    validation_data=test_generator,
    validation_steps=test_generator.samples // batch_size,
    epochs=epochs
)

"""Instantiate Models"""

# Instantiate models
vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
inception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

vgg16_model = create_model(vgg16_base)
inception_model = create_model(inception_base)
resnet_model = create_model(resnet_base)

"""

Train the Models"""

from tensorflow.keras.optimizers import RMSprop, Adam, SGD, Adagrad

optimizer_vgg16_scratch = RMSprop(learning_rate=0.001)
optimizer_vgg16_transfer = Adam(learning_rate=0.001)
optimizer_inception = SGD(learning_rate=0.001, momentum=0.9)
optimizer_resnet = Adagrad(learning_rate=0.001)

# Compile VGG16 from scratch
vgg16_model_scratch.compile(loss='categorical_crossentropy',
                            optimizer=optimizer_vgg16_scratch,
                            metrics=['accuracy'])

history_inception = train_model(inception_model, train_generator, test_generator)

history_resnet = train_model(resnet_model, train_generator, test_generator)

# Train the models
history_vgg16 = train_model(vgg16_model, train_generator, test_generator)

# Assuming you have already created vgg16_model, inception_model, and resnet_model
# and you have defined your train_generator and test_generator

# Train the models
history_vgg16 = vgg16_model.fit(train_generator,
                                steps_per_epoch=len(train_generator),
                                epochs=10,
                                validation_data=test_generator,
                                validation_steps=len(test_generator),
                                verbose=1)

history_inception = inception_model.fit(train_generator,
                                        steps_per_epoch=len(train_generator),
                                        epochs=10,
                                        validation_data=test_generator,
                                        validation_steps=len(test_generator),
                                        verbose=1)

history_resnet = resnet_model.fit(train_generator,
                                  steps_per_epoch=len(train_generator),
                                  epochs=10,
                                  validation_data=test_generator,
                                  validation_steps=len(test_generator),
                                  verbose=1)
